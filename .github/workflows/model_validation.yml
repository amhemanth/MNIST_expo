name: Model Requirements Validation

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: pip
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Run Model Tests
        id: run_tests
        run: python -m unittest test_model.py -v

      - name: Create Results Processing Script
        if: always()
        run: |
          cat > process_results.py << 'ENDOFSCRIPT'
#!/usr/bin/env python3
import json
try:
    with open("test_results.json", "r") as f:
        results = json.load(f)

    arch = results.get("architecture", {})
    perf = results.get("performance", {})

    # Architecture Section
    param_count = arch.get("parameter_count", None)
    if param_count is not None:
        print(f"- Parameter Count: {param_count:,} {'✓' if param_count < 20000 else '❌ Exceeds 20k'}")
    else:
        print("- Parameter Count: N/A")

    batch_norm_count = arch.get("batch_norm_count", None)
    if batch_norm_count is not None:
        print(f"- Batch Normalization Layers: {batch_norm_count} ✓")
    else:
        print("- Batch Normalization Layers: N/A")

    dropout_probs = arch.get("dropout_probs", [None])
    if dropout_probs[0] is not None:
        print(f"- Dropout Probability: {dropout_probs[0]} ✓")
    else:
        print("- Dropout Probability: N/A")

    has_fc = arch.get("has_fc", None)
    if has_fc is not None:
        print(f"- Architecture: {'FC' if has_fc else 'GAP'} ✓")
    else:
        print("- Architecture: N/A")

    output_shape = arch.get("output_shape", None)
    if output_shape is not None:
        print(f"- Output Shape: {output_shape} ✓")
    else:
        print("- Output Shape: N/A")

    print("\n### Performance Tests")

    device = perf.get("device", None)
    if device is not None:
        print(f"- Device: {device}")
    else:
        print("- Device: N/A")

    inference_time = perf.get("inference_time", None)
    if inference_time is not None:
        print(f"- Inference Time: {inference_time}ms ✓")
    else:
        print("- Inference Time: N/A")

    valid_probabilities = perf.get("valid_probabilities", None)
    if valid_probabilities is not None:
        print(f"- Valid Probabilities: {'✓' if valid_probabilities else '❌'}")
    else:
        print("- Valid Probabilities: N/A")

    model_stability = perf.get("model_stability", None)
    if model_stability is not None:
        print(f"- Model Stability: {'✓' if model_stability else '❌'}")
    else:
        print("- Model Stability: N/A")
except Exception as e:
    print(f"Error processing test results: {e}")
ENDOFSCRIPT
          chmod +x process_results.py

      - name: Generate Test Report
        if: always()
        run: |
          if [ -f test_results.json ]; then
            echo "## Model Validation Results" >> "$GITHUB_STEP_SUMMARY"
            echo "### Architecture Tests" >> "$GITHUB_STEP_SUMMARY"
            ./process_results.py >> "$GITHUB_STEP_SUMMARY"
          else
            echo "⚠️ No test results file found" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@main
        with:
          name: test-results
          path: test_results.json
          retention-days: 90