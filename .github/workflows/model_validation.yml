name: Model Requirements Validation

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: pip
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Run Model Tests
        id: run_tests
        run: python -m unittest test_model.py -v

      - name: Create Simple Report Script
        if: always()
        run: |
          python3 -c "import json, os;\ntry:\n    if os.path.exists('test_results.json'):\n        with open('test_results.json', 'r') as f:\n            results = json.load(f)\n        with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:\n            summary.write('## Model Validation Results\n')\n            summary.write('### Architecture Tests\n')\n            arch = results.get('architecture', {})\n            perf = results.get('performance', {})\n            param_count = arch.get('parameter_count', None)\n            summary.write(f'- Parameter Count: {param_count:,} {'\u2713' if param_count < 20000 else '\u274C Exceeds 20k'}\n' if param_count is not None else '- Parameter Count: N/A\n')\n            batch_norm_count = arch.get('batch_norm_count', None)\n            summary.write(f'- Batch Normalization Layers: {batch_norm_count} \u2713\n' if batch_norm_count is not None else '- Batch Normalization Layers: N/A\n')\n            dropout_probs = arch.get('dropout_probs', [None])\n            summary.write(f'- Dropout Probability: {dropout_probs[0]} \u2713\n' if dropout_probs[0] is not None else '- Dropout Probability: N/A\n')\n            has_fc = arch.get('has_fc', None)\n            summary.write(f'- Architecture: {'FC' if has_fc else 'GAP'} \u2713\n' if has_fc is not None else '- Architecture: N/A\n')\n            output_shape = arch.get('output_shape', None)\n            summary.write(f'- Output Shape: {output_shape} \u2713\n' if output_shape is not None else '- Output Shape: N/A\n')\n            summary.write('\n### Performance Tests\n')\n            device = perf.get('device', None)\n            summary.write(f'- Device: {device}\n' if device is not None else '- Device: N/A\n')\n            inference_time = perf.get('inference_time', None)\n            summary.write(f'- Inference Time: {inference_time}ms \u2713\n' if inference_time is not None else '- Inference Time: N/A\n')\n            valid_probabilities = perf.get('valid_probabilities', None)\n            summary.write(f'- Valid Probabilities: {'\u2713' if valid_probabilities else '\u274C'}\n' if valid_probabilities is not None else '- Valid Probabilities: N/A\n')\n            model_stability = perf.get('model_stability', None)\n            summary.write(f'- Model Stability: {'\u2713' if model_stability else '\u274C'}\n' if model_stability is not None else '- Model Stability: N/A\n')\n    else:\n        with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:\n            summary.write('⚠️ No test results file found\n')\nexcept Exception as e:\n    with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:\n        summary.write(f'Error processing test results: {e}\n')"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@main
        with:
          name: test-results
          path: test_results.json
          retention-days: 90