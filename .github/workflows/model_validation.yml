# Name of the workflow that appears in GitHub Actions tab
name: Model Requirements Validation

# Defines when the workflow will run
on: [push, pull_request]  # Triggers on both push events and pull requests

jobs:
  test:  # Job ID
    runs-on: ubuntu-latest  # Specifies the runner environment
    
    # Add permissions for writing to the workspace
    permissions:
      contents: write
      # Add permission for uploading artifacts
      actions: write
    
    steps:
    # Step 1: Check out the repository code
    - uses: actions/checkout@v3  # Latest stable version
    
    # Step 2: Set up Python environment with pip caching
    - name: Set up Python
      uses: actions/setup-python@v4  # Latest stable version
      with:
        python-version: '3.8'  # Specify Python version
        cache: pip  # Enable pip caching
        cache-dependency-path: |
          requirements.txt
        
    # Step 3: Install project dependencies
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip  # Upgrade pip
        pip install -r requirements.txt  # Install project requirements
        
    # Step 4: Prepare data directory
    - name: Create data directory
      run: |
        mkdir -p data
        
    # Step 5: Run all tests
    - name: Run Model Tests
      id: run_tests
      run: |
        python -m unittest test_model.py -v
        
    # Step 6: Generate Test Report from JSON results
    - name: Generate Test Report
      if: always()
      run: |
        if [ -f test_results.json ]; then
          echo "## Model Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "### Architecture Tests" >> $GITHUB_STEP_SUMMARY
          
          # Read and format architecture results
          python3 - <<EOF
          import json
          with open('test_results.json') as f:
              results = json.load(f)
          
          arch = results['architecture']
          perf = results['performance']
          
          # Architecture Section
          print(f"- Parameter Count: {arch['parameter_count']:,} ✓" if arch['parameter_count'] < 20000 else "- Parameter Count: ❌ Exceeds 20k")
          print(f"- Batch Normalization Layers: {arch['batch_norm_count']} ✓")
          print(f"- Dropout Probability: {arch['dropout_probs'][0]} ✓")
          print(f"- Architecture: {'FC' if arch['has_fc'] else 'GAP'} ✓")
          print(f"- Output Shape: {arch['output_shape']} ✓")
          print("\n### Performance Tests")
          print(f"- Device: {perf['device']}")
          print(f"- Inference Time: {perf['inference_time']}ms ✓")
          print(f"- Valid Probabilities: {'✓' if perf['valid_probabilities'] else '❌'}")
          print(f"- Model Stability: {'✓' if perf['model_stability'] else '❌'}")
          EOF
          
        else
          echo "⚠️ No test results file found" >> $GITHUB_STEP_SUMMARY
        fi

    # Step 7: Upload test results as artifact
    - name: Upload Test Results
      if: always()
      uses: actions/upload-artifact@v3  # Latest stable version
      with:
        name: test-results
        path: test_results.json
        retention-days: 90  # Keep artifacts for 90 days 