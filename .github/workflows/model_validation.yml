name: Model Requirements Validation

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: write

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: pip
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Run Model Tests
        id: run_tests
        run: python -m unittest test_model.py -v

      - name: Create Simple Report Script
        if: always()
        run: |
          python3 -c """
import json
import os

try:
    if os.path.exists('test_results.json'):
        with open('test_results.json', 'r') as f:
            results = json.load(f)
        
        # Write to summary
        with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:
            summary.write('## Model Validation Results\n')
            summary.write('### Architecture Tests\n')
            
            arch = results.get('architecture', {})
            perf = results.get('performance', {})
            
            # Architecture Section
            param_count = arch.get('parameter_count', None)
            if param_count is not None:
                summary.write(f\"- Parameter Count: {param_count:,} {'✓' if param_count < 20000 else '❌ Exceeds 20k'}\\n\")
            else:
                summary.write('- Parameter Count: N/A\n')
            
            batch_norm_count = arch.get('batch_norm_count', None)
            if batch_norm_count is not None:
                summary.write(f\"- Batch Normalization Layers: {batch_norm_count} ✓\\n\")
            else:
                summary.write('- Batch Normalization Layers: N/A\n')
            
            dropout_probs = arch.get('dropout_probs', [None])
            if dropout_probs[0] is not None:
                summary.write(f\"- Dropout Probability: {dropout_probs[0]} ✓\\n\")
            else:
                summary.write('- Dropout Probability: N/A\n')
            
            has_fc = arch.get('has_fc', None)
            if has_fc is not None:
                summary.write(f\"- Architecture: {'FC' if has_fc else 'GAP'} ✓\\n\")
            else:
                summary.write('- Architecture: N/A\n')
            
            output_shape = arch.get('output_shape', None)
            if output_shape is not None:
                summary.write(f\"- Output Shape: {output_shape} ✓\\n\")
            else:
                summary.write('- Output Shape: N/A\n')
            
            summary.write('\n### Performance Tests\n')
            
            device = perf.get('device', None)
            if device is not None:
                summary.write(f\"- Device: {device}\\n\")
            else:
                summary.write('- Device: N/A\n')
            
            inference_time = perf.get('inference_time', None)
            if inference_time is not None:
                summary.write(f\"- Inference Time: {inference_time}ms ✓\\n\")
            else:
                summary.write('- Inference Time: N/A\n')
            
            valid_probabilities = perf.get('valid_probabilities', None)
            if valid_probabilities is not None:
                summary.write(f\"- Valid Probabilities: {'✓' if valid_probabilities else '❌'}\\n\")
            else:
                summary.write('- Valid Probabilities: N/A\n')
            
            model_stability = perf.get('model_stability', None)
            if model_stability is not None:
                summary.write(f\"- Model Stability: {'✓' if model_stability else '❌'}\\n\")
            else:
                summary.write('- Model Stability: N/A\n')
    else:
        with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:
            summary.write('⚠️ No test results file found\n')
except Exception as e:
    with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as summary:
        summary.write(f\"Error processing test results: {e}\\n\")
"""

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@main
        with:
          name: test-results
          path: test_results.json
          retention-days: 90